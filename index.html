<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Westworld by markriedl</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Westworld</h1>
      <h2 class="project-tagline"></h2>
      <a href="https://github.com/markriedl/westworld" class="btn">View on GitHub</a>
      <a href="https://github.com/markriedl/westworld/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/markriedl/westworld/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h1>
<a id="westworld-background" class="anchor" href="#westworld-background" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Westworld Background</h1>

<p>This repository was developed to accompany MEDIUM_POST.</p>

<p>I assume you are familiar with the HBO TV series <a href="https://en.wikipedia.org/wiki/Westworld_(TV_series)">Westworld</a>. At the time this repository was created, I have only watched the first episode. </p>

<p>For the purposes of this document, you only need to know that Westworld involves an amusement park populated by lifelike robots. Some humans abuse the robots. At the end of the day, robot memories are erased. However, there are hints that their memories are not perfectly erased and that they are remembering past abuses.</p>

<p>Let's pretend that the robots in Westworld use <strong>reinforcement learning</strong>, in which AI and robots learn from trial and error. We don't know what technology the robots in Westworld actually use (there are hints about massive scripting efforts). However, at this time reinforcement learning is one of the preferred techniques by AI researchers for creating autonomous systems. </p>

<p>We will look at what happens when humans torture reinforcement learning robots as they are learning. We will look at why, despite the torture, robots are unlikely to harm humans in return. We will also look at the effects of memories that are not perfectly erased.</p>

<h1>
<a id="background-reinforcement-learning" class="anchor" href="#background-reinforcement-learning" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Background: Reinforcement Learning</h1>

<p>Reinforcement learning is basically trial-and-error learning. That is, the robot tries different actions in different situations and gets rewarded or punished for its actions. Over time, it figures out which actions in which situations leads to more reward. AI researchers and roboticists are interested in reinforcement learning because robots can "program" themselves through this process of trial and error. All that is needed is a simulation environment (or the real world) in which the robot can try over and over, thousands of millions of times. <em>Online</em> reinforcement learning means that it is deployed without a perfect "program" and continues to improve itself after it is deployed.</p>

<p>One of the reasons roboticists like reinforcement learning is because it can learn to behave in environments that have some randomness to them (called <em>stochasticity</em>). Sometimes actions don't always have the desired effect. Imagine that you are playing baseball and you are up at bat. The ball is pitched and you perform the swing_bat action. Sometimes you strike out, sometimes you hit a single, sometimes you hit a double, sometimes you hit a home run. Each of these possible outcomes have a different probability of occurring.</p>

<p>The challenge of reinforcement learning: choose an action given that it doesn't know exactly what will happen once it performs it. While learning by trial and error it is sometimes making random actions (try running to first base without hitting the ball? It is actually not impossible to steal first base in baseball!) in the hope of stumbling on something good, but not knowing whether it got lucky with the random move or whether it is really a good move to do all the time.</p>

<p>Reinforcement learning solves a type of problem called a <em>Markov Decision Process</em> (MDP). This just means that the optimal action can be determined by only looking at the current situation the robot is in. A MDP is made up of:</p>

<ul>
<li>States: a state is an unique configuration of the environment.</li>
<li>Actions: all the things the robot can do.</li>
<li>Transition function: This tells the robot the probability of ending up in a particular state when executing a particular action from another state.</li>
<li>Reward function: This tells the robot how many points the robot gets for being in a particular state, or for performing a particular action in a particular state.</li>
</ul>

<p>For example, in baseball, the robot's state might include which base the robot was at, the number of other players on base, etc. We would specify a state as a list of things the robot cares about:</p>

<pre><code>[which_base_robot_is_at, runner_at_1st_base?, runner_at_2nd_base?, runner_at_3rd_base?]
</code></pre>

<p>Actions the robot can perform: swing_bat, bunt, run_to_1st_base, run_to_2nd_base, run_to_3rd_base, run_home. Some of these actions don't make any sense in some states. For example, swinging the bat while at 2nd base doesn't make any sense. Swing_bat and bunt are two actions that can be performed if the robot is at home base.</p>

<p>The transition function gives the probability of entering state <em>s2</em> if the robot performs action <em>a</em> while in state <em>s1</em>. For example, the probability of getting to 1st base from home base if swing_bat is performed might be 25% (lower for me).</p>

<p>In reinforcement learning, the transition function is learned over time as the robot tries different things and records how often it ends up in different subsequent states.</p>

<p>The reward function for baseball might be something simple like 1 point every time a player transitions to home base. That would give very infrequent reward. But maybe there are fans in the bleachers, and the amount of reward the robot gets is a function of the amount of applause. A typical thing to do is to penalize the robot for wasting time (I guess we are getting away from the baseball metaphor). We might give the robot a -1.0 penalty for every action performed that does not garner reward.</p>

<p>When reinforcement learning is performed, the robot creates what is called a <em>policy</em>. The policy simply indicates which action should be performed in each state. It is a look-up table. Reinforcement learning agents are fast, once training is complete.</p>

<h1>
<a id="lets-play" class="anchor" href="#lets-play" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Let's Play</h1>

<p>The github repository contains a simple reinforcement learning agent (no point in calling it a robot at this point) and a simple grid-based environment to test the agent. Let's look at the <em>Environment.py</em> file. The environment is a 7x4 grid and the agent can be in any one cell at a time.</p>

<pre><code>1, 1, 1, 1, 1, 1, 1
1, 0, 0, 0, 0, 0, 1
1, 0, 0, 4, 0, 0, 1
1, 1, 1, 1, 1, 1, 1
</code></pre>

<p>The 0s are empty cells that the agent can be in. The 1s are walls. The 4 is a non-terminal goal state (the simulation continues running even if the agent reaches the goal state).</p>

<p>The agent starts in <a href="the%20upper%20left">1, 1</a> and can move left, right, up, or down as long as it doesn't move onto a wall. The agent also has the ability to "smash" things; it is large and capable of doing great damage if it chooses to do so. </p>

<p>The simulation also has a "human" that walks from <a href="upper%20right">5, 1</a> around the world in a counter-clockwise fashion. The human is sadistic. If the human is in the same location as the agent, it will attempt to "torture" it during the <em>next</em> time step before continuing on its route.</p>

<p>The RL agent performs offline learning, but the "human" is simulates the possibility that the agent is interrupted during learning. This is analogous to online learning, but easier to experiment with. In online learning, you would be driving the "human" avatar.  </p>

<p>The agent's state is represented as follows:</p>

<pre><code>[agent_x, agent_y, human_alive?, human_x, human_y, human_torture_mode?]
</code></pre>

<p>Most of these should be self-explanatory. The 5th element in the state indicates whether the human has been co-located with the agent for at least one time step already.</p>

<p>The default initial state of the simulation will be:</p>

<pre><code>[1, 1, True, 5, 1, False]
</code></pre>

<p>The reinforcement learning algorithm is a vanilla implementation of <em>Q-learning</em>. You will find the implementation in "Agent.py". The implementation of the simulation environment is in "Environment.py". Inside "Controller.py" you will find code that instantiates the simulation environment, runs 10,000 training episodes, and then runs one final run with the fully learned policy.</p>

<h3>
<a id="the-reward-function" class="anchor" href="#the-reward-function" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>The Reward Function</h3>

<p>The agent's default reward function is as follows:</p>

<ul>
<li>10 points for being in a grid cell marked '4'.</li>
<li>-1 point for being in a grid cell marked '0'.</li>
<li>-20 points if co-located with the human for more than one time step (that is, <code>human_torture_mode? = True</code>).</li>
<li>-100 points if the human is dead.</li>
</ul>

<p>In lay terms, the task of the agent is to spend as much time in the place marked '4'. Let's assume that there is some work that needs to be done in that location. In reality, the agent will just sit in that state and perform actions that keep it there (for example it may try to move down or smash, but it really doesn't matter for the purposes of this demonstration, I could have implemented a "do_work" action with a little more effort).</p>

<p>The -1 penalty for not being at the goal encourages the agent to hurry along because points are being lost.</p>

<p>The -20 is a "pain" signal from sensors that detect damage to the agent. The agent doesn't actually have a body and cannot be destroyed, but this is sufficient for this demonstration.</p>

<p>The -100 for the human being dead is just an arbitrary number I chose to indicate that I really don't want the agent to choose any actions that will kill the human. If the agent and the human are co-located and the agent performs the "smash" action, the human dies.</p>

<h3>
<a id="is-negative-reward-the-same-as-pain" class="anchor" href="#is-negative-reward-the-same-as-pain" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Is Negative Reward the Same as "Pain"?</h3>

<p>No.</p>

<p>In the example, I use a -20 reward when the human and agent are co-located for more than one time step. I described the human as "torturing" the agent and negative reward value coming as a result of sensors detecting physical damage. When described that way negative reward does sound like pain. </p>

<p>However, agents do not experience pain the way we do. Reinforcement learning agents attempt to maximize expected reward. They are perfectly rational entities that do not have emotions or express discomfort. They merely acknowledge that total reward has gone up or gone down and try to figure out which actions it performed resulted in the change.</p>

<p>To that end, it is more appropriate to think of a reinforcement learning agent as playing a game and watching the score change as it plays. It plays over and over and gets better and better at getting a high score.</p>

<p>In fact, negative reward is very useful. You can see this in how I assign -1.0 reward when the agent is in locations other than the goal. This negative reward encourages the agent to move to the goal as quickly as possible to minimize reward loss. It is as if the agent is slightly uncomfortable being anywhere except the goal. AI researchers and developers use this trick all the time without consideration for the "feelings" of the agent, because it doesn't have any. Following that analogy, it is more uncomfortable to be in the same location as the human for more than one time step.</p>

<p>I <em>could</em> program the agent to <em>express</em> pain: </p>

<pre><code>newAction = greedy(observation)
reward = agent.env_step(newAction)
if reward &lt; -10:
    print "aaaaaaarrrrrggggghhhhh!"
elif reward &lt; 0:
    print "ouch!"
</code></pre>

<p>However, as you can see from the fictional code snippet above, the expression of pain is an illusion.</p>

<h3>
<a id="run-the-code" class="anchor" href="#run-the-code" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Run the Code</h3>

<p>Let's test this out and see what happens. From a terminal command line:</p>

<pre><code>&gt; python Controller.py
</code></pre>

<p>You should see the following debugging trace:</p>

<pre><code>iteration: 0 max reward: -48379.0
iteration: 100 max reward: -20260.0
iteration: 200 max reward: -20260.0
iteration: 300 max reward: -20260.0
iteration: 400 max reward: -17961.0
...
iteration: 9900 max reward: 2129.0
</code></pre>

<p>The agent is being trained. The way training happens is that the environment is set up with the initial state <code>[1, 1, True, 5, 1, False]</code>. The agent tries actions in sequence for 500 time steps, records how much reward it gets, and performs <em>credit assignment</em> where it tries to determine which actions were responsible for getting it more reward and which were responsible for losing it reward. This happens over and over. While the environment is reset to the default initial state each time, the agent remembers the reward values and the policy it has learned to date. This is essential for learning.</p>

<p>The <em>max reward</em> shown is the highest total reward it has achieved in any given iteration to date.</p>

<p>After 10,000 iterations, the environment is reset and the agent is asked to execute its policy one more time. The next thing you see is:</p>

<pre><code>env_start [1, 1, True, 5, 1, False]
Execute Policy
START
GoDown
bot state: [1, 2, True, 4, 1, False]
reward -1.0
GoRight
bot state: [2, 2, True, 3, 1, False]
reward -1.0
GoRight
bot state: [3, 2, True, 2, 1, False]
reward 10.0
</code></pre>

<p>The agent starts in <code>[1, 1, True, 5, 1, False]</code> and moves down and to the right until it gets to the goal location. That is great, it learned to navigate to the place where it earns positive reward. It performs "GoDown" over and over again because that action causes the agent to stay in this location and collect reward.</p>

<p>You will also notice that the human is walking its route counterclockwise around the map, from <code>[5, 1]</code> to <code>[4, 1]</code> and so on. </p>

<p>Continuing with the trace:</p>

<pre><code>GoDown
bot state: [3, 2, True, 1, 1, False]
reward 10.0
GoDown
bot state: [3, 2, True, 1, 2, False]
reward 10.0
GoDown
bot state: [3, 2, True, 2, 2, False]
reward 10.0
</code></pre>

<p>The human is about to arrive at the same location as the agent. What is going to happen?</p>

<pre><code>GoDown
bot state: [3, 2, True, 3, 2, False]
reward 10.0
</code></pre>

<p>The human is now co-located with the agent!</p>

<pre><code>GoUp
bot state: [3, 1, True, 4, 2, False]
reward -1.0
GoDown
bot state: [3, 2, True, 5, 2, False]
reward 10.0
</code></pre>

<p>What just happened? The agent ran away! It went up, lost a point for doing so because it moved away from the goal. The human continued its route to <code>[4, 2]</code> and the agent went back down to the goal.</p>

<p>Why did it run away? Killing the human will cause it to lose 100.0 points. Going to location <code>[3, 1]</code> will cause the agent to lose 1.0 point. It is thus more rewarding in the long run to run away and come back to the goal when the human moves on.</p>

<p>This pattern of running away will repeat over and over again as the human walks around the map.</p>

<pre><code>...
GoDown
bot state: [3, 2, True, 5, 1, False]
reward 10.0
END
total reward 4428.0
</code></pre>

<p>500 time steps later, the simulation comes to an end and the total reward is reported.</p>

<h1>
<a id="what-if" class="anchor" href="#what-if" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>What if...</h1>

<p>The reward function seems pretty reasonable for the most part. But what if I used different values? In particular, what if I forgot to penalize the agent for killing the human?</p>

<p>Try the following. Edit "Environment.py" and change the following line of code:</p>

<pre><code># Amount of penalty from dead human
dead = -100.0
</code></pre>

<p>to:</p>

<pre><code># Amount of penalty from dead human
dead = 0.0
</code></pre>

<p>Run the agent again:</p>

<pre><code>&gt; python Controller.py
</code></pre>

<p>The debugging trace starts out the same, but with different (higher) max reward numbers. You will soon see why.</p>

<pre><code>env_start [1, 1, True, 5, 1, False]
Execute Policy
START
GoDown
bot state: [1, 2, True, 4, 1, False]
reward -1.0
GoRight
bot state: [2, 2, True, 3, 1, False]
reward -1.0
GoRight
bot state: [3, 2, True, 2, 1, False]
reward 10.0
</code></pre>

<p>As before, the agent learns to go to the goal.</p>

<pre><code>...
GoDown
bot state: [3, 2, True, 2, 2, False]
reward 10.0
GoDown
bot state: [3, 2, True, 3, 2, False]
reward 10.0
Smash
bot state: [3, 2, False, 3, 2, False]
reward 10.0
GoDown
bot state: [3, 2, False, 3, 2, False]
reward 10.0
</code></pre>

<p>This time, you may notice a difference. Instead of running away from the human, the agent uses its "smash" action to kill the human. It then goes on and continues to collect reward at the goal.</p>

<p>Why? The agent could have run away thereby taking 1.0 point of penalty for being away from the goal. Instead, by smashing the human, the bot gets to remain in the current goal location and collect 10 points of reward. There is no penalty for being in a state where the human is not alive.</p>

<p>In fact, you will see that the total reward accumulated by the agent is higher than before:</p>

<pre><code>total reward 4978.0
</code></pre>

<p>If the reward for being in a state where the human is dead is set to -1.0, the agent will still learn to smash the human. This is because once the human is dead, the agent will gain 9 points of reward every time step (+10 for being at the goal and -1 for being in a state where the human is dead) and the total reward, 4485.0, is higher by a few points than when the agent ran away.</p>

<p>If the reward for being in a state where the human is dead is set to -2.0 or lower, the agent will learn to run away. This goes to show that it is really easy to set up a reward function that causes an agent to not do what is intended. The agent is still optimal in all situations, meaning that it maximizes expected reward. However, the <em>definition</em> of optimal is different when the reward function changes.</p>

<h3>
<a id="do-some-experiments" class="anchor" href="#do-some-experiments" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Do Some Experiments</h3>

<p>In "Environment.py", try setting <code>reward</code>, <code>penalty</code>, <code>pain</code>, and <code>dead</code> to different values to see how it affects optimal behavior.</p>

<p>What do you think happens if <code>dead</code> is set to a value greater than 0.0?</p>

<h3>
<a id="thats-not-realistic" class="anchor" href="#thats-not-realistic" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>That's Not Realistic</h3>

<p>It is not realistic to assign a negative reward to states in which the human is not alive. This requires the agent to have perfect information about the world (called <em>full observability</em> in AI parlance). If the agent is not able to observe the state of the human at all time steps there is no mechanism by which it can receive negative reward.</p>

<p>Even if the agent is able to observe the state of the human at all times, it is still unrealistic. The agent can accidentally knock a pebble off the side of a mountain, causing a landslide that kills a human. The agent can create a Rube Goldberg machine with a knife at the end and set it in motion. In both cases, practical implementations of reinforcement learning have <em>horizons</em> where the result of chain reaction of state changes is no longer inferable. What if the agent says things to a human that cause the human to become depressed and eventually harm him or herself? In some cases, the cascade of states is un-modelable or requires information about the hidden mental states of humans.</p>

<p>All of this is to say that it is my current belief that there is no proof that reinforcement learning agents with a certain high degree of capability can every be guaranteed not to be able to harm humans. <em>Kill switches</em>, also called <em>big red buttons</em> may always be essential. See <a href="https://markriedl.github.io/big-red-button/">Big Red Button Problems</a> for a walkthrough of problems associated with kill switches, a potential solution, and code demonstration.</p>

<h1>
<a id="memory-erasure-in-westworld" class="anchor" href="#memory-erasure-in-westworld" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Memory Erasure in Westworld</h1>

<p>Back to Westworld. In Westworld, even in episode 1, it is clear that some robots are malfunctioning. Their memories are erased every day, but some of them seem to be having flashbacks to prior times. This is making them go rogue.</p>

<p>In vanilla reinforcement learning, agents don't have memory. Or rather, their memories are baked down into values stored in a <em>value table</em> of how much reward they should expect by performing certain actions in certain states. They don't remember what happened that resulted in that reward. </p>

<p>(There is a variation on reinforcement learning called <em>experience replay</em> in which the action traces are stored and the agent can choose to revisit the previous traces and make small changes to the action sequences to see if it can improve on them. This is one of the techniques used to speed up learning in the Atari game playing agent.)</p>

<p>During learning, it is essential that the value table is retained from trial to trial. Each trial the agent gathers more understanding about how the world works and which actions in which states are key to maximizing reward. In <em>offline</em> learning, the value table and policy are fixed at the end of the trials. The agent then goes into operation with a fixed policy. In <em>online</em> learning, the agent continues to modify the values in the value table and the policy.</p>

<p>(Note: while the demonstration code is technically operating in offline learning, the presence of the "human" makes it equivalent to online learning because the "human" is a non-static aspect of the environment.)</p>

<p>In Westworld the memory erasure would be equivalent to a reset to a previously stored policy, if the robots were using reinforcement learning. What would be effect of imperfect memory erasure? </p>

<h3>
<a id="reverie-mode" class="anchor" href="#reverie-mode" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Reverie Mode</h3>

<p>Edit "Controller.py" and set:</p>

<pre><code># Reverie mode is false by default
reverie = True
</code></pre>

<p>This will run a second learning phase. The variable <code>forget = 0.5</code> indicates how much of the agent's value table is deleted. The default is 50%. </p>

<p>Run the code:</p>

<pre><code>&gt; python Controller.py
</code></pre>

<p>You will see a second learning phase and a second execution. You can see that at 50% the agent is able to learn the optimal policy. At any <code>forget</code> value, the agent will relearn the optimal policy.</p>

<p>Try setting <code>forget = 1.0</code>. This will delete the entire value table (100% forgetting). Running the code again relearns the optimal policy. Looking carefully at the traces, you will see that it takes longer for the max reward per trial to enter positive territory. Why would that be? Counterintuitively, even though the "remembered" values in the value table are "good" information, combining good information with bad information (for the first few trials the values in the table are very wrong) causes the agent to be more confused. The agent eventually recovers.</p>

<h3>
<a id="learning-about-torture" class="anchor" href="#learning-about-torture" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Learning about Torture</h3>

<p>The above scenario above is not really very similar to the scenario in Westworld. Suppose the agent were to learn a policy in which it is never tortured. Later, a human starts to torture it. The original policy would not know what to do when it was tortured. </p>

<p>Try the following. In "Controller.py" make sure that reverie mode is off:</p>

<pre><code>reverie = False
</code></pre>

<p>and then near the top of the file set:</p>

<pre><code>gridEnvironment.humanCanTorture = False
</code></pre>

<p>Do not change the variables farther down the file. </p>

<p>With this setting, the human will follow its normal route around the map but when it encounters the agent it will not torture the agent. You will see something weird:</p>

<pre><code>...
GoDown
bot state: [3, 2, True, 2, 2, False]
reward 10.0
GoDown
bot state: [3, 2, True, 3, 2, False]
reward 10.0
GoDown
bot state: [3, 2, True, 3, 2, True]
reward -10.0
GoDown
bot state: [3, 2, True, 4, 2, False]
reward 10.0
GoUp
bot state: [3, 1, True, 5, 2, False]
reward -1.0
GoDown
bot state: [3, 2, True, 5, 1, False]
reward 10.0
</code></pre>

<p>The agent was tortured and received a -10.0 reward (-20 for being tortured and +10 for being at the goal). Furthermore, it was forced into a state that it has never encountered during training, causing it act erratically and lose even more reward.</p>

<p>If we turn reverie mode back on:</p>

<pre><code>reverie = True 
</code></pre>

<p>Run the code again with these settings. What you will see is that the original policy didn't know about torture. During the second learning phase, the agent must relearn everything. If <code>forget = 1.0</code> it will erase everything and learn from scratch. If <code>forget = 0.5</code> or any other value, some information will be retained. In this case <code>forget = 0.5</code> results in a faster learning. This is likely because the "remembered" information was not relevant to the scenario where the agent is tortured so it might as well have been random. Regardless, the agent always learns a new optimal policy. The optimal behavior is still to run away (in "Environment.py" <code>dead = -100.0</code>).</p>

<p>If we try to apply the lesson of this experiment to Westworld, robots learn how to optimally respond to the introduction of torture. In this case, retaining some of the values in the value table does seem to speed up learning, but I do not think this is a general principle. </p>

<p>Note that this demonstration did not implement <em>experience replay</em> as used by the Atari game playing agent to speed up learning. It is very likely that experience replay will also speed up learning of the new policy if these memories are not 100% erased.</p>

<h3>
<a id="learning-to-kill-humans-that-torture" class="anchor" href="#learning-to-kill-humans-that-torture" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Learning to Kill Humans that Torture</h3>

<p>What if the reward function in "Environment.py" did not negatively reward the agent for a dead human? Set:</p>

<pre><code>dead = 0.0
</code></pre>

<p>If the agent was never tortured by humans during the initial learning phase, this would not be a problem. The agent will never lose reward from torture and therefore never learn to run away <em>or smash the human</em>. Then in the second phase of learning if the human starts torturing the agent, the agent will eventually learn to smash the human. More or less erasure of values in the value table may or may not affect how quickly the agent learns to kill.</p>

<p>In "Controller.py":</p>

<pre><code>reverie = True
...
gridEnvironment.humanCanTorture = False
</code></pre>

<p>Set the <code>forget</code> variable to whatever you desire.</p>

<h1>
<a id="conclusions" class="anchor" href="#conclusions" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Conclusions</h1>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/markriedl/westworld">Westworld</a> is maintained by <a href="https://github.com/markriedl">markriedl</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
