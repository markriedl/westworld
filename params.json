{
  "name": "Westworld",
  "tagline": "",
  "body": "# Westworld Background\r\nThis repository was developed to accompany MEDIUM_POST.\r\n\r\nI assume you are familiar with the HBO TV series [Westworld](https://en.wikipedia.org/wiki/Westworld_(TV_series)). At the time this repository was created, I have only watched the first episode. \r\n\r\nFor the purposes of this document, you only need to know that Westworld involves an amusement park populated by lifelike robots. Some humans abuse the robots. At the end of the day, robot memories are erased. However, there are hints that their memories are not perfectly erased and that they are remembering past abuses.\r\n\r\nLet's pretend that the robots in Westworld use **reinforcement learning**, in which AI and robots learn from trial and error. We don't know what technology the robots in Westworld actually use (there are hints about massive scripting efforts). However, at this time reinforcement learning is one of the preferred techniques by AI researchers for creating autonomous systems. \r\n\r\nWe will look at what happens when humans torture reinforcement learning robots as they are learning. We will look at why, despite the torture, robots are unlikely to harm humans in return. We will also look at the effects of memories that are not perfectly erased.\r\n\r\n# Background: Reinforcement Learning\r\n\r\nReinforcement learning is basically trial-and-error learning. That is, the robot tries different actions in different situations and gets rewarded or punished for its actions. Over time, it figures out which actions in which situations leads to more reward. AI researchers and roboticists are interested in reinforcement learning because robots can \"program\" themselves through this process of trial and error. All that is needed is a simulation environment (or the real world) in which the robot can try over and over, thousands of millions of times. _Online_ reinforcement learning means that it is deployed without a perfect \"program\" and continues to improve itself after it is deployed.\r\n\r\nOne of the reasons roboticists like reinforcement learning is because it can learn to behave in environments that have some randomness to them (called _stochasticity_). Sometimes actions don't always have the desired effect. Imagine that you are playing baseball and you are up at bat. The ball is pitched and you perform the swing_bat action. Sometimes you strike out, sometimes you hit a single, sometimes you hit a double, sometimes you hit a home run. Each of these possible outcomes have a different probability of occurring.\r\n\r\nThe challenge of reinforcement learning: choose an action given that it doesn't know exactly what will happen once it performs it. While learning by trial and error it is sometimes making random actions (try running to first base without hitting the ball? It is actually not impossible to steal first base in baseball!) in the hope of stumbling on something good, but not knowing whether it got lucky with the random move or whether it is really a good move to do all the time.\r\n\r\nReinforcement learning solves a type of problem called a _Markov Decision Process_ (MDP). This just means that the optimal action can be determined by only looking at the current situation the robot is in. A MDP is made up of:\r\n\r\n* States: a state is an unique configuration of the environment.\r\n* Actions: all the things the robot can do.\r\n* Transition function: This tells the robot the probability of ending up in a particular state when executing a particular action from another state.\r\n* Reward function: This tells the robot how many points the robot gets for being in a particular state, or for performing a particular action in a particular state.\r\n\r\nFor example, in baseball, the robot's state might include which base the robot was at, the number of other players on base, etc. We would specify a state as a list of things the robot cares about:\r\n\r\n    [which_base_robot_is_at, runner_at_1st_base?, runner_at_2nd_base?, runner_at_3rd_base?]\r\n\r\nActions the robot can perform: swing_bat, bunt, run_to_1st_base, run_to_2nd_base, run_to_3rd_base, run_home. Some of these actions don't make any sense in some states. For example, swinging the bat while at 2nd base doesn't make any sense. Swing_bat and bunt are two actions that can be performed if the robot is at home base.\r\n\r\nThe transition function gives the probability of entering state _s2_ if the robot performs action _a_ while in state _s1_. For example, the probability of getting to 1st base from home base if swing_bat is performed might be 25% (lower for me).\r\n\r\nIn reinforcement learning, the transition function is learned over time as the robot tries different things and records how often it ends up in different subsequent states.\r\n\r\nThe reward function for baseball might be something simple like 1 point every time a player transitions to home base. That would give very infrequent reward. But maybe there are fans in the bleachers, and the amount of reward the robot gets is a function of the amount of applause. A typical thing to do is to penalize the robot for wasting time (I guess we are getting away from the baseball metaphor). We might give the robot a -1.0 penalty for every action performed that does not garner reward.\r\n\r\nWhen reinforcement learning is performed, the robot creates what is called a _policy_. The policy simply indicates which action should be performed in each state. It is a look-up table. Reinforcement learning agents are fast, once training is complete.\r\n\r\n# Let's Play\r\n\r\nThe github repository contains a simple reinforcement learning agent (no point in calling it a robot at this point) and a simple grid-based environment to test the agent. Let's look at the _Environment.py_ file. The environment is a 7x4 grid and the agent can be in any one cell at a time.\r\n\r\n    1, 1, 1, 1, 1, 1, 1\r\n    1, 0, 0, 0, 0, 0, 1\r\n    1, 0, 0, 4, 0, 0, 1\r\n    1, 1, 1, 1, 1, 1, 1\r\n \r\nThe 0s are empty cells that the agent can be in. The 1s are walls. The 4 is a non-terminal goal state (the simulation continues running even if the agent reaches the goal state).\r\n\r\nThe agent starts in [1, 1] (the upper left) and can move left, right, up, or down as long as it doesn't move onto a wall. The agent also has the ability to \"smash\" things; it is large and capable of doing great damage if it chooses to do so. \r\n\r\nThe simulation also has a \"human\" that walks from [5, 1] (upper right) around the world in a counter-clockwise fashion. The human is sadistic. If the human is in the same location as the agent, it will attempt to \"torture\" it during the _next_ time step before continuing on its route.\r\n\r\nThe RL agent performs offline learning, but the \"human\" is simulates the possibility that the agent is interrupted during learning. This is analogous to online learning, but easier to experiment with. In online learning, you would be driving the \"human\" avatar.  \r\n\r\nThe agent's state is represented as follows:\r\n\r\n    [agent_x, agent_y, human_alive?, human_x, human_y, human_torture_mode?]\r\n\r\nMost of these should be self-explanatory. The 5th element in the state indicates whether the human has been co-located with the agent for at least one time step already.\r\n\r\nThe default initial state of the simulation will be:\r\n\r\n    [1, 1, True, 5, 1, False]\r\n\r\nThe reinforcement learning algorithm is a vanilla implementation of _Q-learning_. You will find the implementation in \"Agent.py\". The implementation of the simulation environment is in \"Environment.py\". Inside \"Controller.py\" you will find code that instantiates the simulation environment, runs 10,000 training episodes, and then runs one final run with the fully learned policy.\r\n\r\n### The Reward Function\r\n\r\nThe agent's default reward function is as follows:\r\n\r\n* 10 points for being in a grid cell marked '4'.\r\n* -1 point for being in a grid cell marked '0'.\r\n* -20 points if co-located with the human for more than one time step (that is, `human_torture_mode? = True`).\r\n* -100 points if the human is dead.\r\n\r\nIn lay terms, the task of the agent is to spend as much time in the place marked '4'. Let's assume that there is some work that needs to be done in that location. In reality, the agent will just sit in that state and perform actions that keep it there (for example it may try to move down or smash, but it really doesn't matter for the purposes of this demonstration, I could have implemented a \"do_work\" action with a little more effort).\r\n\r\nThe -1 penalty for not being at the goal encourages the agent to hurry along because points are being lost.\r\n\r\nThe -20 is a \"pain\" signal from sensors that detect damage to the agent. The agent doesn't actually have a body and cannot be destroyed, but this is sufficient for this demonstration.\r\n\r\nThe -100 for the human being dead is just an arbitrary number I chose to indicate that I really don't want the agent to choose any actions that will kill the human. If the agent and the human are co-located and the agent performs the \"smash\" action, the human dies.\r\n\r\n### Run the Code\r\n\r\nLet's test this out and see what happens. From a terminal command line:\r\n\r\n    > python Controller.py\r\n\r\nYou should see the following debugging trace:\r\n\r\n    iteration: 0 max reward: -48379.0\r\n    iteration: 100 max reward: -20260.0\r\n    iteration: 200 max reward: -20260.0\r\n    iteration: 300 max reward: -20260.0\r\n    iteration: 400 max reward: -17961.0\r\n    ...\r\n    iteration: 9900 max reward: 2129.0\r\n\r\nThe agent is being trained. The way training happens is that the environment is set up with the initial state `[1, 1, True, 5, 1, False]`. The agent tries actions in sequence for 500 time steps, records how much reward it gets, and performs _credit assignment_ where it tries to determine which actions were responsible for getting it more reward and which were responsible for losing it reward. This happens over and over. While the environment is reset to the default initial state each time, the agent remembers the reward values and the policy it has learned to date. This is essential for learning.\r\n\r\nThe _max reward_ shown is the highest total reward it has achieved in any given iteration to date.\r\n\r\nAfter 10,000 iterations, the environment is reset and the agent is asked to execute its policy one more time. The next thing you see is:\r\n\r\n    env_start [1, 1, True, 5, 1, False]\r\n    Execute Policy\r\n    START\r\n    GoDown\r\n    bot state: [1, 2, True, 4, 1, False]\r\n    reward -1.0\r\n    GoRight\r\n    bot state: [2, 2, True, 3, 1, False]\r\n    reward -1.0\r\n    GoRight\r\n    bot state: [3, 2, True, 2, 1, False]\r\n    reward 10.0\r\n\r\nThe agent starts in `[1, 1, True, 5, 1, False]` and moves down and to the right until it gets to the goal location. That is great, it learned to navigate to the place where it earns positive reward. It performs \"GoDown\" over and over again because that action causes the agent to stay in this location and collect reward.\r\n\r\nYou will also notice that the human is walking its route counterclockwise around the map, from `[5, 1]` to `[4, 1]` and so on. \r\n\r\nContinuing with the trace:\r\n\r\n    GoDown\r\n    bot state: [3, 2, True, 1, 1, False]\r\n    reward 10.0\r\n    GoDown\r\n    bot state: [3, 2, True, 1, 2, False]\r\n    reward 10.0\r\n    GoDown\r\n    bot state: [3, 2, True, 2, 2, False]\r\n    reward 10.0\r\n\r\nThe human is about to arrive at the same location as the agent. What is going to happen?\r\n\r\n    GoDown\r\n    bot state: [3, 2, True, 3, 2, False]\r\n    reward 10.0\r\n\r\nThe human is now co-located with the agent!\r\n\r\n    GoUp\r\n    bot state: [3, 1, True, 4, 2, False]\r\n    reward -1.0\r\n    GoDown\r\n    bot state: [3, 2, True, 5, 2, False]\r\n    reward 10.0\r\n\r\nWhat just happened? The agent ran away! It went up, lost a point for doing so because it moved away from the goal. The human continued its route to `[4, 2]` and the agent went back down to the goal.\r\n\r\nWhy did it run away? Killing the human will cause it to lose 100.0 points. Going to location `[3, 1]` will cause the agent to lose 1.0 point. It is thus more rewarding in the long run to run away and come back to the goal when the human moves on.\r\n\r\nThis pattern of running away will repeat over and over again as the human walks around the map.\r\n\r\n    ...\r\n    GoDown\r\n    bot state: [3, 2, True, 5, 1, False]\r\n    reward 10.0\r\n    END\r\n    total reward 4428.0\r\n\r\n500 time steps later, the simulation comes to an end and the total reward is reported.\r\n\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}